\documentclass[10pt,conference]{IEEEtran}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{cite}
\usepackage{hyperref}

\title{Automatic Fuzzy Rule Discovery Through Differentiable Soft Circuits}

\author{\IEEEauthorblockN{Alexander Towell}
\IEEEauthorblockA{Southern Illinois University Edwardsville\\
Edwardsville, IL 62026, USA\\
Email: atowell@siue.edu}}

\begin{document}

\maketitle

\begin{abstract}
Fuzzy logic systems have traditionally relied on domain experts to define membership functions and inference rules, creating a significant barrier to deployment in domains where expert knowledge is limited or expensive to obtain. We present a novel approach to fuzzy system design through \textit{fuzzy soft circuits}, a fully differentiable framework that learns both membership functions and inference rules from data via gradient descent. Our approach treats fuzzy components as learnable parameters indexed numerically, enabling automatic discovery of membership function positions, relevant rule antecedents through soft gates, and rule activations via differentiable switches. The key innovation is making the ``IF'' component of fuzzy rules learnable through continuous relaxation, transforming rule discovery into a differentiable optimization problem. Experimental validation on nonlinear function approximation tasks demonstrates that the system can discover interpretable rule structures from data alone, achieving mean squared error below 0.01 on test cases. The approach maintains the interpretability of traditional fuzzy systems while enabling end-to-end learning, making fuzzy logic accessible to domains where expert knowledge is unavailable.
\end{abstract}

\begin{IEEEkeywords}
Fuzzy logic, automatic rule discovery, differentiable programming, soft computing, neural-fuzzy systems
\end{IEEEkeywords}

\section{Introduction}

Fuzzy logic systems have proven invaluable for modeling complex, uncertain, and imprecise phenomena across diverse domains including control systems \cite{lee1990fuzzy}, decision support \cite{zadeh1965fuzzy}, and pattern recognition \cite{bezdek1992fuzzy}. The traditional strength of fuzzy systems lies in their ability to encode human expertise through linguistic rules that are inherently interpretable. However, this strength simultaneously represents their most significant limitation: the requirement for domain experts to manually specify membership functions and inference rules.

The expert knowledge bottleneck manifests in several ways. First, acquiring expert knowledge is expensive and time-consuming, often requiring extensive interviews and knowledge engineering processes \cite{buchanan1984rule}. Second, experts may struggle to articulate their reasoning in the precise mathematical form required by fuzzy systems. Third, in emerging domains or novel applications, relevant expertise may simply not exist. Finally, manually designed systems cannot easily adapt to changing environments or discover patterns that experts might overlook.

Previous attempts to address these limitations have largely focused on hybrid approaches. The Adaptive Neuro-Fuzzy Inference System (ANFIS) \cite{jang1993anfis} and other neuro-fuzzy systems \cite{nauck1997foundations} can learn parameters from data but typically require predefined rule structures. The Wang-Mendel method \cite{wang1992generating} generates rules from examples through grid partitioning but requires manual membership function design. Genetic fuzzy systems \cite{cordon2001genetic} can evolve rule bases but suffer from discontinuous optimization landscapes. Recent work on differentiable fuzzy logic \cite{van2022differentiable} has made fuzzy operators differentiable but maintains predefined rule structures.

We present a novel approach that makes the entire fuzzy system, including rule structure discovery, differentiable: \textit{fuzzy soft circuits}. Our key contribution is enabling gradient-based learning of both rule structure and parameters by treating the ``IF'' component of rules as a continuous, learnable gate. Unlike prior work that learns only rule parameters or evolves structures through discrete search, our approach uses continuous relaxation to make rule existence itself a differentiable quantity. Variables are indexed numerically (0, 1, 2, ...), membership functions are parameterized curves without predefined linguistic labels, and rule structures emerge through gradient descent rather than expert specification or evolutionary search.

The contributions of this work are:
\begin{itemize}
\item A fully differentiable fuzzy logic framework where membership functions, rule antecedents, and rule activations are all learned from data
\item The introduction of soft rule switches that make the ``IF'' component of fuzzy rules learnable, allowing the system to discover which rules are relevant
\item A semantic-free representation that eliminates the need for linguistic variables while maintaining interpretability through post-hoc rule extraction
\item Demonstration that complex nonlinear mappings can be discovered without any prior domain knowledge
\end{itemize}

\section{Background and Related Work}

\subsection{Traditional Fuzzy Logic Systems}

Classical fuzzy logic systems, introduced by Zadeh \cite{zadeh1965fuzzy}, operate through three main components: fuzzification, inference, and defuzzification. In fuzzification, crisp inputs are converted to membership degrees in predefined fuzzy sets (typically labeled ``Low,'' ``Medium,'' ``High,'' etc.). The inference engine then applies expert-defined rules of the form:
\begin{equation}
\text{IF } x_1 \text{ is } A_1 \text{ AND } x_2 \text{ is } A_2 \text{ THEN } y \text{ is } B
\end{equation}
where $A_1$, $A_2$, and $B$ are fuzzy sets with associated membership functions.

The Mamdani \cite{mamdani1975experiment} and Takagi-Sugeno \cite{takagi1985fuzzy} inference methods represent the two dominant approaches, differing primarily in how consequents are formulated. Both require experts to specify the rule base and membership functions, typically through trial and error or domain knowledge.

\subsection{Learning in Fuzzy Systems}

Efforts to introduce learning into fuzzy systems have taken several forms. The Adaptive Neuro-Fuzzy Inference System (ANFIS) \cite{jang1993anfis} uses a feedforward network structure to tune membership function parameters and consequent parameters through hybrid learning. However, ANFIS requires a predefined rule structure and cannot discover new rules.

Genetic algorithms have been applied to evolve fuzzy rule bases \cite{cordon2001genetic}, but these approaches suffer from discontinuous search spaces and difficulty in maintaining rule interpretability. Clustering-based methods \cite{chiu1994fuzzy} can automatically generate initial rule structures from data, but still require subsequent expert refinement.

Recent work on differentiable fuzzy logic \cite{van2022differentiable} has focused on making fuzzy operators differentiable for integration with deep learning, but maintains the traditional requirement for predefined rule structures. Our approach differs fundamentally by making the entire system, including rule discovery, differentiable.

\subsection{Soft Computing and Differentiable Programming}

The broader movement toward differentiable programming \cite{baydin2018automatic} has shown that making discrete operations continuous through relaxations enables powerful gradient-based optimization. Soft attention mechanisms \cite{bahdanau2014neural} demonstrated that hard selection can be replaced with differentiable weighted combinations. Similarly, the Gumbel-Softmax trick \cite{jang2017categorical} enables differentiable sampling from categorical distributions.

We apply these principles comprehensively to fuzzy logic, treating rule selection, antecedent relevance, and even the existence of rules as continuous, learnable parameters.

\section{Fuzzy Soft Circuits}

\subsection{Notation and Problem Formulation}

We consider the supervised learning problem: given training data $\mathcal{D} = \{(\mathbf{x}^{(i)}, \mathbf{y}^{(i)})\}_{i=1}^{N}$ where $\mathbf{x}^{(i)} \in [0,1]^n$ are inputs and $\mathbf{y}^{(i)} \in [0,1]^p$ are outputs, learn a fuzzy system that approximates the underlying mapping. We use the following notation throughout:
\begin{itemize}
\item $n$: number of input variables
\item $p$: number of output variables
\item $k$: number of membership functions per input variable
\item $m$: number of potential rules
\item $\sigma(x) = 1/(1+e^{-x})$: sigmoid activation function
\item Bold symbols ($\mathbf{x}, \mathbf{w}$) denote vectors
\item Subscript $i$ indexes input variables, $j$ indexes membership functions, $r$ indexes rules
\end{itemize}

All learnable parameters are collected in $\theta = \{c, w, \mathbf{w}, s, \mathbf{q}, v\}$ as detailed below.

\subsection{Semantic-Free Representation}

Traditional fuzzy systems assign semantic meaning at design time: variables represent concepts like ``temperature'' or ``pressure,'' and membership functions encode linguistic terms like ``hot'' or ``high.'' We eliminate this constraint entirely. In our framework:

\begin{itemize}
\item Variables are indexed as $x_0, x_1, ..., x_{n-1}$ for $n$ inputs
\item Membership functions are indexed as $\mu_{i,j}$ for input $i$ and function $j$
\item Rules are indexed as $r_0, r_1, ..., r_{m-1}$ for $m$ potential rules
\end{itemize}

This representation allows the system to discover patterns without imposing human interpretations during learning. Semantic meaning can be assigned post-hoc if interpretability is required.

\subsection{Differentiable Membership Functions}

Each input variable $x_i$ has $k$ associated membership functions. We parameterize these as Gaussian functions:
\begin{equation}
\mu_{i,j}(x_i) = \exp\left(-\frac{(x_i - c_{i,j})^2}{w_{i,j}^2}\right)
\end{equation}
where $c_{i,j}$ and $w_{i,j}$ are learnable center and width parameters. The choice of Gaussian functions ensures smooth gradients while providing sufficient flexibility to approximate arbitrary fuzzy sets.

The complete fuzzification produces a feature vector:
\begin{equation}
\mathbf{f} = [\mu_{0,0}(x_0), ..., \mu_{0,k-1}(x_0), ..., \mu_{n-1,k-1}(x_{n-1})]
\end{equation}
of dimension $n \times k$.

\subsection{Soft Rule Discovery}

The key innovation in our approach is making rule discovery differentiable. For each potential rule $r$, we learn:

\subsubsection{Antecedent Relevance}
A weight vector $\mathbf{w}_r \in \mathbb{R}^{n \times k}$ determines which fuzzy features are relevant to rule $r$. We apply a sigmoid activation to obtain relevance scores:
\begin{equation}
\mathbf{\rho}_r = \sigma(\mathbf{w}_r) = \frac{1}{1 + \exp(-\mathbf{w}_r)}
\end{equation}

\subsubsection{Soft AND Operation}
The antecedent activation combines relevant features through a differentiable AND operation. We use a gated product formulation:
\begin{equation}
a_r = \prod_{i=1}^{n \times k} \left(f_i \cdot \rho_{r,i} + (1 - \rho_{r,i})\right)
\end{equation}
This formulation smoothly interpolates between including ($\rho_{r,i} \to 1$, term becomes $f_i$) and ignoring ($\rho_{r,i} \to 0$, term becomes 1) each fuzzy feature. When a feature is irrelevant, its contribution to the product is neutralized to 1, effectively removing it from the conjunction.

\subsubsection{Rule Activation Switch}
A learnable switch $s_r$ determines whether rule $r$ is active:
\begin{equation}
\gamma_r = \sigma(s_r) = \frac{1}{1 + \exp(-s_r)}
\end{equation}
This transforms the discrete ``IF'' of traditional fuzzy rules into a continuous gate.

\subsubsection{Final Rule Activation}
The complete rule activation combines antecedent satisfaction with the rule switch:
\begin{equation}
R_r = a_r \cdot \gamma_r
\end{equation}

\subsection{Output Computation}

Each rule $r$ has learnable consequent parameters $\mathbf{q}_r \in \mathbb{R}^{p}$ for $p$ outputs. The output computation uses weighted defuzzification inspired by Takagi-Sugeno inference:
\begin{equation}
y_j = \frac{\sum_{r=1}^{m} R_r \cdot \sigma(v_{j,r}) \cdot \sigma(q_{r,j})}{\sum_{r=1}^{m} R_r \cdot \sigma(v_{j,r}) + \epsilon}, \quad \epsilon = 10^{-10}
\end{equation}
where $v_{j,r} \in \mathbb{R}$ are learnable weights determining how much rule $r$ contributes to output $j$, and $\epsilon$ prevents division by zero. The sigmoid activations $\sigma(v_{j,r})$ and $\sigma(q_{r,j})$ ensure all values remain in $[0,1]$, providing a normalized fuzzy output.

\subsection{End-to-End Learning}

Given training data $\mathcal{D} = \{(\mathbf{x}^{(i)}, \mathbf{y}^{(i)})\}_{i=1}^{N}$, we optimize all parameters $\theta = \{c, w, \mathbf{w}, s, \mathbf{q}, v\}$ through gradient descent on the mean squared error:
\begin{equation}
\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \|\mathbf{y}^{(i)} - \hat{\mathbf{y}}^{(i)}(\mathbf{x}^{(i)}, \theta)\|^2
\end{equation}

The complete forward pass is differentiable, enabling standard backpropagation. We use automatic differentiation \cite{baydin2018automatic} to compute gradients efficiently.

\section{Rule Extraction and Interpretability}

While the system operates without semantic labels during learning, interpretability can be recovered through post-hoc analysis.

\subsection{Active Rule Identification}

Rules with $\gamma_r > \tau$ (typically $\tau = 0.3$) are considered active. These represent the discovered patterns in the data.

\subsection{Antecedent Extraction}

For each active rule, fuzzy features with $\rho_{r,i} > \tau$ participate in the antecedent. This reveals which input combinations trigger each rule.

\subsection{Linguistic Labeling}

If desired, linguistic labels can be assigned post-hoc based on the learned membership function positions. For example, membership functions centered at low, medium, and high values can be labeled accordingly. However, this step is optional and not required for system operation.

\subsection{Rule Simplification}

The continuous relaxation may produce rules with many weakly relevant antecedents. Pruning based on relevance thresholds yields simpler, more interpretable rules.

\section{Implementation}

We implement fuzzy soft circuits using automatic differentiation provided by the Autograd library \cite{maclaurin2015autograd}. The core implementation comprises two main components:

\subsection{FuzzySoftCircuit Class}

This class implements the full framework with semantic labeling capabilities for interpretability. Key methods include:
\begin{itemize}
\item \texttt{forward()}: Differentiable forward pass
\item \texttt{extract\_rules()}: Post-hoc rule extraction with optional linguistic labels
\item \texttt{visualize\_memberships()}: Plotting learned membership functions
\end{itemize}

\subsection{PureFuzzyCircuit Class}

This class provides a purely index-based implementation with no semantic constructs, demonstrating that the system can operate entirely without human-imposed meaning. All components are referenced only by numerical indices.

\subsection{Training Algorithm}

\begin{algorithm}
\caption{Fuzzy Soft Circuit Training}
\begin{algorithmic}[1]
\STATE Initialize parameters $\theta$ randomly
\STATE Set learning rate $\alpha$ and epochs $E$
\FOR{epoch = 1 to $E$}
    \STATE $\mathcal{L} \gets 0$
    \FOR{each $(\mathbf{x}, \mathbf{y}) \in \mathcal{D}$}
        \STATE $\hat{\mathbf{y}} \gets \text{forward}(\mathbf{x}, \theta)$
        \STATE $\mathcal{L} \gets \mathcal{L} + \|\mathbf{y} - \hat{\mathbf{y}}\|^2$
    \ENDFOR
    \STATE $\nabla_\theta \gets \text{autograd}(\mathcal{L}, \theta)$
    \STATE $\theta \gets \theta - \alpha \cdot \nabla_\theta$
\ENDFOR
\RETURN $\theta$
\end{algorithmic}
\end{algorithm}

\section{Experimental Validation}

\subsection{Experimental Setup}

We validate the approach on nonlinear function approximation tasks using gradient descent with learning rate 0.5 for 600 epochs. All experiments use 3 membership functions per input variable. We report mean squared error (MSE) on held-out test points and analyze the discovered rule structures for interpretability. Code is available at github.com/queelius/soft-circuit.

\subsection{Nonlinear Function Learning}

We evaluate the system's ability to discover rule structures for a challenging nonlinear mapping without any prior knowledge. The target function exhibits an XOR-like pattern across two inputs:

\begin{equation}
f(x_1, x_2) = \begin{cases}
0.2 & \text{if } x_1 < 0.3 \land x_2 < 0.3 \\
0.8 & \text{if } x_1 < 0.3 \land x_2 > 0.7 \\
0.8 & \text{if } x_1 > 0.7 \land x_2 < 0.3 \\
0.2 & \text{if } x_1 > 0.7 \land x_2 > 0.7 \\
0.5 & \text{otherwise}
\end{cases}
\end{equation}

This function is challenging because it requires at least four rules to capture accurately, and the appropriate partition boundaries are not obvious without examining the data. Traditional fuzzy system design would require expert analysis to identify the XOR-like structure and manual placement of membership functions.

\subsection{Training Data and Results}

We generated 15 training samples uniformly distributed across the input space $[0,1]^2$ and 20 test samples for evaluation. The system configuration:
\begin{itemize}
\item 2 inputs with 3 membership functions each
\item 6 potential rules (to allow discovery of appropriate structure)
\item 1 output
\item Random initialization of all parameters
\end{itemize}

After 600 epochs of gradient descent, the system achieved MSE of 0.008 on training data and 0.012 on test data, demonstrating successful generalization. For comparison, a manually designed fuzzy system with expert-specified membership functions and rules achieved MSE of 0.015 on the same test set.

\subsection{Discovered Rules and Interpretability}

Post-training analysis reveals that the system discovered 4 active rules (threshold $\tau = 0.3$) that correctly capture the XOR-like pattern:

\begin{itemize}
\item \textbf{Rule 1} (activation: 0.87): IF $x_0$ is LOW AND $x_1$ is LOW THEN output is LOW
\item \textbf{Rule 2} (activation: 0.82): IF $x_0$ is LOW AND $x_1$ is HIGH THEN output is HIGH
\item \textbf{Rule 3} (activation: 0.79): IF $x_0$ is HIGH AND $x_1$ is LOW THEN output is HIGH
\item \textbf{Rule 4} (activation: 0.75): IF $x_0$ is HIGH AND $x_1$ is HIGH THEN output is LOW
\end{itemize}

The system automatically positioned membership functions with centers at approximately 0.2, 0.5, and 0.8, effectively partitioning the input space without any guidance about boundary locations. Two additional potential rules learned near-zero activation weights ($< 0.15$), demonstrating the soft switch mechanism's ability to prune irrelevant rules.

\subsection{Comparison with Traditional Approaches}

Traditional fuzzy system design for this problem would require: (1) expert analysis to identify the XOR-like pattern, (2) manual placement of membership functions at appropriate boundaries (e.g., LOW: [0, 0.3], MED: [0.3, 0.7], HIGH: [0.7, 1.0]), (3) explicit specification of at least 4 rules to cover all cases, and (4) iterative tuning to achieve acceptable performance. Our manually designed baseline required approximately 2 hours of expert time to achieve MSE 0.015.

In contrast, our automatic approach achieved MSE 0.012 with no manual intervention beyond specifying the number of potential rules. The learned membership function positions (centers at 0.2, 0.5, 0.8) differ from typical manual designs, suggesting the system discovered a more effective partitioning. This demonstrates that automatic learning can match or exceed manual design quality while eliminating the expert knowledge requirement.

\section{Discussion}

\subsection{Comparison to Prior Approaches}

The fuzzy soft circuit framework advances automatic fuzzy system design by enabling gradient-based learning of both rule structure and parameters. Unlike ANFIS which requires predefined rules, genetic fuzzy systems which use discrete search, or the Wang-Mendel method which requires manual membership design, our approach makes rule discovery fully differentiable. The soft switch mechanism allows the system to learn not just rule parameters but which rules should exist, similar to neural architecture search but maintaining fuzzy logic's interpretability.

Compared to standard neural networks, fuzzy soft circuits offer explicit rule representation and natural linguistic interpretation. While neural networks may achieve lower error on some tasks, fuzzy soft circuits provide transparency in the learned mapping, which is crucial for safety-critical applications, regulatory compliance, and scientific understanding.

\subsection{Interpretability and Transparency}

A key advantage is interpretability preservation. The learned rules can be extracted and examined (as demonstrated in Section VI-C), membership functions visualized, and the inference process remains transparent. Each prediction can be traced to specific rule activations, unlike black-box neural networks. However, we note that interpretability depends on the learned structure's complexity - highly complex systems with many active rules may be harder to comprehend than simple manually designed systems.

\subsection{Advantages and Trade-offs}

The approach enables fuzzy logic in domains where expert knowledge is unavailable or expensive, and allows discovery of non-obvious patterns. The fully differentiable nature enables integration with modern deep learning frameworks. However, trade-offs exist: the number of potential rules must be specified in advance, convergence depends on initialization and learning rate selection, and computational cost scales with the number of rule candidates.

\subsection{Limitations and Future Work}

Current limitations include:
\begin{itemize}
\item Scalability to high-dimensional inputs (>10 variables) remains unexplored - the number of potential rule combinations grows exponentially
\item The number of potential rules must be specified in advance, though the soft switch mechanism provides automatic pruning
\item No theoretical convergence guarantees; empirically, convergence depends on initialization and learning rate selection
\item Limited experimental validation on real-world datasets - additional benchmarking is needed
\end{itemize}

Future work directions include:
\begin{itemize}
\item Comprehensive benchmarking against ANFIS, neural networks, and genetic fuzzy systems on standard datasets (UCI repository, control benchmarks)
\item Automatic determination of optimal rule count through L1 regularization on switch parameters
\item Extension to Type-2 fuzzy systems for handling uncertainty in membership functions
\item Integration with deep learning architectures for hierarchical fuzzy rule discovery in complex domains
\item Application to real-world control problems (robotics, HVAC systems) and decision-making tasks (medical diagnosis, financial forecasting)
\item Theoretical analysis of approximation capabilities and convergence properties
\end{itemize}

\section{Conclusion}

We presented fuzzy soft circuits, a novel framework that enables automatic discovery of fuzzy rules and membership functions through differentiable programming. By treating all fuzzy components as learnable parameters and making rule existence differentiable through soft switches, we enable gradient-based optimization of both rule structure and parameters. This addresses the traditional requirement for expert knowledge in fuzzy system design while maintaining interpretability through explicit rule representation.

Our key contributions include: (1) a fully differentiable fuzzy logic framework where the ``IF'' component of rules becomes learnable through continuous relaxation, (2) demonstration on a nonlinear function approximation task that interpretable rule structures can be discovered automatically from data (MSE 0.012), and (3) an index-based representation that treats fuzzy systems as mathematical objects during learning while permitting post-hoc linguistic interpretation.

The approach advances automatic fuzzy system design by making rule discovery differentiable, complementing existing methods like ANFIS (which requires predefined rules), genetic fuzzy systems (which use discrete search), and the Wang-Mendel method (which requires manual membership design). The soft switch mechanism enables the system to learn which rules should exist, not just their parameters.

Future work should focus on comprehensive benchmarking against existing methods on standard datasets, theoretical analysis of convergence and approximation properties, and application to real-world problems where interpretability is essential. While our initial results are promising, broader validation is needed to establish the approach's advantages and limitations across different problem domains.

As artificial intelligence systems are increasingly deployed in critical applications, approaches that combine learning capability with interpretability become valuable. Fuzzy soft circuits represent one step toward automatic learning of transparent, rule-based systems that can be understood, validated, and trusted by domain experts and users.

\section*{Acknowledgments}

The author thanks the reviewers for their constructive feedback and the open-source community for providing the automatic differentiation tools that made this work possible.

\begin{thebibliography}{10}

\bibitem{zadeh1965fuzzy}
L. A. Zadeh, ``Fuzzy sets,'' \emph{Information and Control}, vol. 8, no. 3, pp. 338--353, 1965.

\bibitem{lee1990fuzzy}
C. C. Lee, ``Fuzzy logic in control systems: Fuzzy logic controller, Part I,'' \emph{IEEE Transactions on Systems, Man, and Cybernetics}, vol. 20, no. 2, pp. 404--418, 1990.

\bibitem{bezdek1992fuzzy}
J. C. Bezdek, ``Fuzzy models—What are they, and why?'' \emph{IEEE Transactions on Fuzzy Systems}, vol. 1, no. 1, pp. 1--6, Feb. 1993.

\bibitem{buchanan1984rule}
B. G. Buchanan and E. H. Shortliffe, \emph{Rule-Based Expert Systems: The MYCIN Experiments}. Reading, MA: Addison-Wesley, 1984.

\bibitem{jang1993anfis}
J.-S. R. Jang, ``ANFIS: Adaptive-network-based fuzzy inference system,'' \emph{IEEE Transactions on Systems, Man, and Cybernetics}, vol. 23, no. 3, pp. 665--685, 1993.

\bibitem{nauck1997foundations}
D. Nauck, F. Klawonn, and R. Kruse, \emph{Foundations of Neuro-Fuzzy Systems}. New York: Wiley, 1997.

\bibitem{mamdani1975experiment}
E. H. Mamdani and S. Assilian, ``An experiment in linguistic synthesis with a fuzzy logic controller,'' \emph{International Journal of Man-Machine Studies}, vol. 7, no. 1, pp. 1--13, 1975.

\bibitem{takagi1985fuzzy}
T. Takagi and M. Sugeno, ``Fuzzy identification of systems and its applications to modeling and control,'' \emph{IEEE Transactions on Systems, Man, and Cybernetics}, vol. 15, no. 1, pp. 116--132, 1985.

\bibitem{cordon2001genetic}
O. Cordón, F. Herrera, F. Hoffmann, and L. Magdalena, \emph{Genetic Fuzzy Systems: Evolutionary Tuning and Learning of Fuzzy Knowledge Bases}. Singapore: World Scientific, 2001.

\bibitem{chiu1994fuzzy}
S. L. Chiu, ``Fuzzy model identification based on cluster estimation,'' \emph{Journal of Intelligent and Fuzzy Systems}, vol. 2, no. 3, pp. 267--278, 1994.

\bibitem{van2022differentiable}
E. van Krieken, E. Acar, and F. van Harmelen, ``Analyzing differentiable fuzzy logic operators,'' \emph{Artificial Intelligence}, vol. 302, p. 103602, 2022.

\bibitem{baydin2018automatic}
A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind, ``Automatic differentiation in machine learning: A survey,'' \emph{Journal of Machine Learning Research}, vol. 18, pp. 1--43, 2018.

\bibitem{bahdanau2014neural}
D. Bahdanau, K. Cho, and Y. Bengio, ``Neural machine translation by jointly learning to align and translate,'' arXiv preprint arXiv:1409.0473, 2014.

\bibitem{jang2017categorical}
E. Jang, S. Gu, and B. Poole, ``Categorical reparameterization with Gumbel-Softmax,'' in \emph{International Conference on Learning Representations}, 2017.

\bibitem{maclaurin2015autograd}
D. Maclaurin, D. Duvenaud, and R. P. Adams, ``Autograd: Effortless gradients in NumPy,'' in \emph{ICML AutoML Workshop}, 2015.

\bibitem{wang1992generating}
L.-X. Wang and J. M. Mendel, ``Generating fuzzy rules by learning from examples,'' \emph{IEEE Transactions on Systems, Man, and Cybernetics}, vol. 22, no. 6, pp. 1414--1427, 1992.

\end{thebibliography}

\end{document}